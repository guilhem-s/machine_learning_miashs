{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"kobe.csv\")\n",
    "data = df.dropna() # Suppression des lignes(les tirs) dont nous ne connaissons pas l'issue(marqué ou manqué)\n",
    "qualitative_vars = [\"action_type\", \"combined_shot_type\", \"game_event_id\", \"game_id\", \n",
    "                    \"period\", \"playoffs\", \"season\", \"shot_type\",\n",
    "                    \"shot_zone_area\", \"shot_zone_basic\", \"shot_zone_range\", \"team_name\", \"game_date\", \"matchup\", \"opponent\"]\n",
    "quantitative_vars = [col for col in df.columns if col not in qualitative_vars + [\"shot_made_flag\", \"shot_id\", 'team_id']]  # garder le shot_id ? \n",
    "X = data[quantitative_vars+qualitative_vars]  # Features : éléments ayant un impact lors de la prise du tir\n",
    "y = data[\"shot_made_flag\"]  # Target : issue du tir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25697,)\n",
      "===== Encoded classes: ['Alley Oop Dunk Shot' 'Alley Oop Layup shot' 'Cutting Layup Shot'\n",
      " 'Driving Bank shot' 'Driving Dunk Shot' 'Driving Finger Roll Layup Shot'\n",
      " 'Driving Finger Roll Shot' 'Driving Floating Bank Jump Shot'\n",
      " 'Driving Floating Jump Shot' 'Driving Hook Shot' 'Driving Jump shot'\n",
      " 'Driving Layup Shot' 'Driving Reverse Layup Shot'\n",
      " 'Driving Slam Dunk Shot' 'Dunk Shot' 'Fadeaway Bank shot'\n",
      " 'Fadeaway Jump Shot' 'Finger Roll Layup Shot' 'Finger Roll Shot'\n",
      " 'Floating Jump shot' 'Follow Up Dunk Shot' 'Hook Bank Shot' 'Hook Shot'\n",
      " 'Jump Bank Shot' 'Jump Hook Shot' 'Jump Shot' 'Layup Shot'\n",
      " 'Pullup Bank shot' 'Pullup Jump shot' 'Putback Dunk Shot'\n",
      " 'Putback Layup Shot' 'Putback Slam Dunk Shot' 'Reverse Dunk Shot'\n",
      " 'Reverse Layup Shot' 'Reverse Slam Dunk Shot' 'Running Bank shot'\n",
      " 'Running Dunk Shot' 'Running Finger Roll Layup Shot'\n",
      " 'Running Finger Roll Shot' 'Running Hook Shot' 'Running Jump Shot'\n",
      " 'Running Layup Shot' 'Running Pull-Up Jump Shot'\n",
      " 'Running Reverse Layup Shot' 'Running Slam Dunk Shot' 'Running Tip Shot'\n",
      " 'Slam Dunk Shot' 'Step Back Jump shot' 'Tip Layup Shot' 'Tip Shot'\n",
      " 'Turnaround Bank shot' 'Turnaround Fadeaway shot'\n",
      " 'Turnaround Finger Roll Shot' 'Turnaround Hook Shot'\n",
      " 'Turnaround Jump Shot']\n",
      "(25697,)\n",
      "===== Encoded classes: ['Bank Shot' 'Dunk' 'Hook Shot' 'Jump Shot' 'Layup' 'Tip Shot']\n",
      "(25697,)\n",
      "===== Encoded classes: [2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52\n",
      " 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
      " 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100\n",
      " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154\n",
      " 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172\n",
      " 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190\n",
      " 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208\n",
      " 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244\n",
      " 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262\n",
      " 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334\n",
      " 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352\n",
      " 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370\n",
      " 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388\n",
      " 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406\n",
      " 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424\n",
      " 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442\n",
      " 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460\n",
      " 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478\n",
      " 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496\n",
      " 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514\n",
      " 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532\n",
      " 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550\n",
      " 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568\n",
      " 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586\n",
      " 587 588 589 590 591 592 593 595 597 598 599 600 601 603 604 605 606 607\n",
      " 610 612 615 616 617 620 623 624 625 628 632 634 639 650 653]\n",
      "(25697,)\n",
      "===== Encoded classes: [20000012 20000019 20000047 ... 49900086 49900087 49900088]\n",
      "(25697,)\n",
      "===== Encoded classes: [1 2 3 4 5 6 7]\n",
      "(25697,)\n",
      "===== Encoded classes: [0 1]\n",
      "(25697,)\n",
      "===== Encoded classes: ['1996-97' '1997-98' '1998-99' '1999-00' '2000-01' '2001-02' '2002-03'\n",
      " '2003-04' '2004-05' '2005-06' '2006-07' '2007-08' '2008-09' '2009-10'\n",
      " '2010-11' '2011-12' '2012-13' '2013-14' '2014-15' '2015-16']\n",
      "(25697,)\n",
      "===== Encoded classes: ['2PT Field Goal' '3PT Field Goal']\n",
      "(25697,)\n",
      "===== Encoded classes: ['Back Court(BC)' 'Center(C)' 'Left Side Center(LC)' 'Left Side(L)'\n",
      " 'Right Side Center(RC)' 'Right Side(R)']\n",
      "(25697,)\n",
      "===== Encoded classes: ['Above the Break 3' 'Backcourt' 'In The Paint (Non-RA)' 'Left Corner 3'\n",
      " 'Mid-Range' 'Restricted Area' 'Right Corner 3']\n",
      "(25697,)\n",
      "===== Encoded classes: ['16-24 ft.' '24+ ft.' '8-16 ft.' 'Back Court Shot' 'Less Than 8 ft.']\n",
      "(25697,)\n",
      "===== Encoded classes: ['Los Angeles Lakers']\n",
      "(25697,)\n",
      "===== Encoded classes: ['1996-11-03' '1996-11-05' '1996-11-06' ... '2016-04-10' '2016-04-11'\n",
      " '2016-04-13']\n",
      "(25697,)\n",
      "===== Encoded classes: ['LAL @ ATL' 'LAL @ BKN' 'LAL @ BOS' 'LAL @ CHA' 'LAL @ CHH' 'LAL @ CHI'\n",
      " 'LAL @ CLE' 'LAL @ DAL' 'LAL @ DEN' 'LAL @ DET' 'LAL @ GSW' 'LAL @ HOU'\n",
      " 'LAL @ IND' 'LAL @ LAC' 'LAL @ MEM' 'LAL @ MIA' 'LAL @ MIL' 'LAL @ MIN'\n",
      " 'LAL @ NJN' 'LAL @ NOH' 'LAL @ NOK' 'LAL @ NOP' 'LAL @ NYK' 'LAL @ OKC'\n",
      " 'LAL @ ORL' 'LAL @ PHI' 'LAL @ PHO' 'LAL @ PHX' 'LAL @ POR' 'LAL @ SAC'\n",
      " 'LAL @ SAS' 'LAL @ SEA' 'LAL @ TOR' 'LAL @ UTA' 'LAL @ UTH' 'LAL @ VAN'\n",
      " 'LAL @ WAS' 'LAL vs. ATL' 'LAL vs. BKN' 'LAL vs. BOS' 'LAL vs. CHA'\n",
      " 'LAL vs. CHH' 'LAL vs. CHI' 'LAL vs. CLE' 'LAL vs. DAL' 'LAL vs. DEN'\n",
      " 'LAL vs. DET' 'LAL vs. GSW' 'LAL vs. HOU' 'LAL vs. IND' 'LAL vs. LAC'\n",
      " 'LAL vs. MEM' 'LAL vs. MIA' 'LAL vs. MIL' 'LAL vs. MIN' 'LAL vs. NJN'\n",
      " 'LAL vs. NOH' 'LAL vs. NOK' 'LAL vs. NOP' 'LAL vs. NYK' 'LAL vs. OKC'\n",
      " 'LAL vs. ORL' 'LAL vs. PHI' 'LAL vs. PHO' 'LAL vs. PHX' 'LAL vs. POR'\n",
      " 'LAL vs. SAC' 'LAL vs. SAN' 'LAL vs. SAS' 'LAL vs. SEA' 'LAL vs. TOR'\n",
      " 'LAL vs. UTA' 'LAL vs. VAN' 'LAL vs. WAS']\n",
      "(25697,)\n",
      "===== Encoded classes: ['ATL' 'BKN' 'BOS' 'CHA' 'CHI' 'CLE' 'DAL' 'DEN' 'DET' 'GSW' 'HOU' 'IND'\n",
      " 'LAC' 'MEM' 'MIA' 'MIL' 'MIN' 'NJN' 'NOH' 'NOP' 'NYK' 'OKC' 'ORL' 'PHI'\n",
      " 'PHX' 'POR' 'SAC' 'SAS' 'SEA' 'TOR' 'UTA' 'VAN' 'WAS']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "encoded_vars = []\n",
    "for categorical_var in data[qualitative_vars].T.values:\n",
    "    print(categorical_var.shape)\n",
    "    encoded_vars.append(label_encoder.fit_transform(categorical_var))\n",
    "    print(f\"===== Encoded classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "\n",
    "# Pour chaque feature catégorielle, créez un LabelEncoder et stockez-le dans le dictionnaire.\n",
    "for feature in qualitative_vars:\n",
    "    le = LabelEncoder()\n",
    "    # Ajuster le LabelEncoder sur les données textuelles.\n",
    "    data.loc[:, feature] = le.fit_transform(data[feature])\n",
    "    \n",
    "    # Stocker le LabelEncoder dans le dictionnaire en utilisant le nom de la feature comme clé.\n",
    "    label_encoders[feature] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {feature: LabelEncoder().fit(data[feature]) for feature in qualitative_vars}\n",
    "# Supposons que vous avez un DataFrame 'data', une liste 'categorical_features'\n",
    "# et un dictionnaire 'label_encoders' qui contient chaque LabelEncoder utilisé pour chaque feature.\n",
    "# 'target' est la variable cible continue.\n",
    "data_copy = data.copy() # Créer une copie explicite du DataFrame\n",
    "# Pour chaque feature catégorielle:\n",
    "for feature in qualitative_vars:\n",
    "    # Obtenir l'objet LabelEncoder pour la feature.\n",
    "    le = label_encoders[feature]\n",
    "    \n",
    "    # Inverser le LabelEncoder pour récupérer les catégories originales.\n",
    "    data_copy[feature + '_original'] = le.inverse_transform(data[feature].astype(int))\n",
    "    \n",
    "    # Calculer les moyennes de la variable cible pour chaque catégorie originale.\n",
    "    means = data_copy.groupby(feature + '_original')['shot_made_flag'].mean()\n",
    "    \n",
    "    # Mappez les moyennes sur les valeurs encodées en utilisant les catégories originales.\n",
    "    data_copy[feature + '_encoded'] = data_copy[feature + '_original'].map(means)\n",
    "\n",
    "# Après cela, 'data' contiendra de nouvelles colonnes avec les encodages basés sur la réponse.\n",
    "# Vous pouvez maintenant utiliser ces nouvelles colonnes dans vos calculs de corrélation ou dans les modèles prédictifs.\n",
    "\n",
    "# Assurez-vous de retirer les colonnes intermédiaires si elles ne sont pas nécessaires.\n",
    "data_copy.drop([feature + '_original' for feature in qualitative_vars], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shot_made_flag et action_type_encoded: 0.37654915475487005\n",
      "shot_made_flag et combined_shot_type_encoded: 0.24652540101945739\n",
      "shot_made_flag et game_event_id_encoded: 0.15738032249532216\n",
      "shot_made_flag et game_id_encoded: 0.24289114153332728\n",
      "shot_made_flag et period_encoded: 0.03836425742636405\n",
      "shot_made_flag et playoffs_encoded: 0.0012568981415939943\n",
      "shot_made_flag et season_encoded: 0.049762515365669266\n",
      "shot_made_flag et shot_type_encoded: 0.12146225672937402\n",
      "shot_made_flag et shot_zone_area_encoded: 0.14862473586264693\n",
      "shot_made_flag et shot_zone_basic_encoded: 0.20732578770279805\n",
      "shot_made_flag et shot_zone_range_encoded: 0.18721452309462025\n",
      "shot_made_flag et team_name_encoded: nan\n",
      "shot_made_flag et game_date_encoded: 0.24289114153332728\n",
      "shot_made_flag et matchup_encoded: 0.05439574857945151\n",
      "shot_made_flag et opponent_encoded: 0.035722169287886456\n"
     ]
    }
   ],
   "source": [
    "# Calculer la matrice de corrélation\n",
    "correlation_matrix = data_copy.corr()\n",
    "\n",
    "# Obtenir la corrélation de 'shot_made_flag' avec toutes les autres variables\n",
    "target_correlation = correlation_matrix['shot_made_flag']\n",
    "\n",
    "# Afficher les corrélations entre 'shot_made_flag' et les colonnes encodées\n",
    "for feature in qualitative_vars:\n",
    "    encoded_feature_name = feature + '_encoded'\n",
    "    if encoded_feature_name in data_copy.columns:\n",
    "        print(f\"shot_made_flag et {encoded_feature_name}: {target_correlation[encoded_feature_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_categorical = np.array(encoded_vars).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Guilem\\anaconda3\\envs\\envguilhem\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(sparse=False, sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(sparse=False, sparse_output=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(sparse=False, sparse_output=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the OneHotEncoder to the categorical variables.\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(data[qualitative_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the categorical variables and convert to DataFrame.\n",
    "one_hot_encoded_data = one_hot_encoder.transform(data[qualitative_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25697, 3952)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to properly align the indices and columns\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded_data, \n",
    "                                  index=data.index, \n",
    "                                  columns=one_hot_encoder.get_feature_names_out(qualitative_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now join the one-hot encoded DataFrame with the target variable `y`.\n",
    "# Ensure that `y` has the same index as `one_hot_encoded_df`.\n",
    "merged_df = pd.concat([one_hot_encoded_df, data['shot_made_flag'].reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the correlation matrix.\n",
    "correlation_matrix = merged_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correlation of all variables with 'shot_made_flag'.\n",
    "correlation_with_target = correlation_matrix.loc[:, 'shot_made_flag']\n",
    "\n",
    "# Show the correlation with the target variable.\n",
    "print(correlation_with_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of `data` to align with the new DataFrame.\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Join the one-hot encoded DataFrame with the target variable `y`.\n",
    "one_hot_encoded_df = pd.concat([one_hot_encoded_df, data[['shot_made_flag']]], axis=1)\n",
    "\n",
    "# Now calculate the correlation matrix.\n",
    "correlation_matrix = one_hot_encoded_df.corr()\n",
    "\n",
    "# Get the correlation of all variables with 'shot_made_flag'.\n",
    "correlation_with_target = correlation_matrix['shot_made_flag']\n",
    "\n",
    "# Show the correlation with the target variable.\n",
    "print(correlation_with_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat                  0.148070\n",
       "loc_x               -0.000848\n",
       "loc_y               -0.148070\n",
       "lon                 -0.000848\n",
       "minutes_remaining    0.028342\n",
       "seconds_remaining    0.030804\n",
       "shot_distance       -0.198242\n",
       "shot_made_flag       1.000000\n",
       "Name: shot_made_flag, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix = data[quantitative_vars+[\"shot_made_flag\"]].corr()\n",
    "correlation_matrix['shot_made_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que les variables qui jouent le plus sont 'lat' 'loc_y' et 'shot distance' On les inscrit dans qualitative_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded_X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['action_type_Alley Oop Dunk Shot',\n",
       "       'action_type_Alley Oop Layup shot',\n",
       "       'action_type_Cutting Finger Roll Layup Shot', ..., 'opponent_UTA',\n",
       "       'opponent_VAN', 'opponent_WAS'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de OneHotEncoder pour les variables catégorielles\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), qualitative_vars)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apprentissage(preprocessor, classifier, X_train, y_train, X_test, y_test):\n",
    "    # Création du pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    # Entraînement du modèle\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Prédiction sur l'ensemble de test\n",
    "    prediction = pipeline.predict(X_test)\n",
    "\n",
    "    # Évaluation du modèle\n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    precision = precision_score(y_test, prediction, zero_division=0)\n",
    "    recall = recall_score(y_test, prediction)    \n",
    "    print(f\"a = {accuracy}, p = {precision}, r = {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix et optimisation de l'algorithme d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Accuracy of our model for i = 41 ============\n",
      "a = 0.5979180854168693, p = 0.5700063211125158, r = 0.3941215034965035\n",
      "========= Accuracy of our model for i = 51 ============\n",
      "a = 0.598307228329604, p = 0.5728725138571894, r = 0.3839597902097902\n",
      "========= Accuracy of our model for i = 61 ============\n",
      "a = 0.6005448000778286, p = 0.5812586445366529, r = 0.3673513986013986\n",
      "========= Accuracy of our model for i = 71 ============\n",
      "a = 0.6014203716314817, p = 0.5854136947218259, r = 0.35871940559440557\n",
      "========= Accuracy of our model for i = 81 ============\n",
      "a = 0.6018581574083082, p = 0.5881816523800839, r = 0.352381993006993\n"
     ]
    }
   ],
   "source": [
    "# Choix de l'algorithme de classification et de ses paramètres\n",
    "for k in range(41, 91, 10):\n",
    "    #classifier = LogisticRegression(max_iter=10000)\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k, p=1)\n",
    "    #classifier = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "    print(f\"========= Accuracy of our model for i = {k} ============\")\n",
    "    apprentissage(preprocessor, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Supposons que X_train et y_train contiennent vos données d'entraînement\n",
    "\n",
    "# Créer un objet de régression Lasso\n",
    "lasso = Lasso(alpha=0.1)  # Vous pouvez ajuster alpha selon vos besoins\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X = data_copy[qualitative_vars+quantitative_vars]\n",
    "y = data_copy[\"shot_made_flag\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.8, random_state=0)\n",
    "\n",
    "\n",
    "# Sélectionner les fonctionnalités les plus importantes\n",
    "selector = SelectFromModel(lasso)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les indices des fonctionnalités sélectionnées\n",
    "selected_features_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Obtenir les noms des fonctionnalités sélectionnées\n",
    "selected_features_names = X_train.columns[selected_features_indices]\n",
    "\n",
    "# Créer un nouveau DataFrame avec seulement les fonctionnalités sélectionnées\n",
    "X_train_selected = X_train[selected_features_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Accuracy of our model for i = 0 ============\n",
      "a = 0.5879462982780426, p = 0.5447849533079048, r = 0.45257867132867136\n"
     ]
    }
   ],
   "source": [
    "for k in range(1):\n",
    "    #classifier = LogisticRegression(max_iter=10000)\n",
    "    classifier = KNeighborsClassifier(n_neighbors=41, p=1)\n",
    "    #classifier = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "    print(f\"========= Accuracy of our model for i = {k} ============\")\n",
    "    apprentissage(preprocessor, classifier, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
